{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![test](../notebooks/images/header3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop goals\n",
    "* Loss functions\n",
    "* Feedforward networks and backpropagation\n",
    "* Activation functions\n",
    "* Deep learning\n",
    "* How to size your model\n",
    "* Rock-paper-scissors predictor\n",
    "\n",
    "# How to get and open this file:\n",
    "1. `git pull`\n",
    "1. `docker-compose build`\n",
    "1. `docker-compose up`\n",
    "1. Open `localhost:8888` in a browser and paste in the token\n",
    "\n",
    "# To stop docker when you are done:\n",
    "1. `control-c` in the termainl window to stop Jupyter\n",
    "1. `docker-compose down`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions\n",
    "\n",
    "All the networks we looked at last time were **feedforward** networks, in which the inputs on the left are used to calculate the final outputs by walking through the neural network graph. Specifically, feedforward networks don't have any circular paths. This is also sometimes called a *Directed Acyclic Graph* or *DAG*.\n",
    "\n",
    "We use **forward propagation** to calculate the output values.\n",
    "\n",
    "1. Start with the input values on the left-hand side ($X_n$).\n",
    "1. The input to a node in the second layer is the weighted sum of the inputs.\n",
    "1. The output of a node in the second layer is found by using the activation function on the weighted sum.\n",
    "1. Repeat until you reach the output layer. \n",
    "\n",
    "<img src=\"../notebooks/images/1hiddenlayer.png\" alt=\"drawing\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions\n",
    "\n",
    "The above prediction algorithm doesn't say anything about obtaining the weights between nodes, although that is pretty obviously a key component of a good neural network! Of course we can make predictions with any random set of weights, but good predictions require training the network. In order to train the network we have to first understand when the network is making good predictions. That requires a **loss function**. Very similar to linear or logistic regressions, and other ML algorithms, the loss function is an equation that measures how far our prediction is from the true answer, summed up for all of our observations. Then the training or fitting of the algorithm becomes an exercise in minimizing the loss function.\n",
    "\n",
    "[Keras documention for loss functions](https://keras.io/losses/)\n",
    "\n",
    "For example, for a regression problem we usually use **mean-squared error** (MSE), just like linear regression:\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "MSE = \\frac{1}{n}\\sum_{i=1}^{n}{(y_i-y_{pred})^2}\n",
    "$$\n",
    "\n",
    "The sum here is over all the observations we are predicting. Depending on circumstances this can be a test set or a single batch during training. If we have a multi-node output, we also have to sum over all the output nodes. Classification tasks have different loss functions I'll talk about a little bit later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network\n",
    "\n",
    "Given an input observation, a set of weights and the network architecture, we use **forward propagation** to calculate the network output and therefore the value of the loss function.\n",
    "\n",
    "There is another procedure, called **backpropagation** that is use to modify the weights of the network to lower the loss function. As the name implies, we use work backwards through the network, using the error at each layer to calculate the error at the previous layer, all the way back to the inputs. \n",
    "\n",
    "We then use the calculated errors, as well as the derivatives of the activation functions to adjust the weights in each layer.\n",
    "\n",
    "This is a very hand-wavey and woefully incomplete description of backpropagation. I wish we had more time to delve into this deeply but I the focus of the class to be more on actually building networks.\n",
    "\n",
    "It's probably best to leave links to resources here. I highly suggest reading some of these.\n",
    "\n",
    "1. Chapter six in https://www.deeplearningbook.org/\n",
    "1. Another online textbook: http://neuralnetworksanddeeplearning.com/chap2.html\n",
    "\n",
    "One note: the error in each layer is dependent on the error in the previous layer. As these tend to become small, the changes made in each successive layer working backwards become smaller. Thus layers near the input in a deep network tend to train slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions\n",
    "\n",
    "[Keras documention for activation functions](https://keras.io/activations/)\n",
    "\n",
    "### Why do we need activation functions:\n",
    "\n",
    "In the previous session we introduced neural networks as a generalization of logistic regression. In the language of deep learning, a logistic regression node uses a sigmoid activation function. There are other activation functions that we can use in certain circumstances.\n",
    "\n",
    "Activation functions have several purposes. The most import one is that activation functions introduce *non-linearity* into the neural network. If we simply used the weighted sum input as the output for all nodes the output would just be a linear combination of the input features, effectively just a complicated linear regression. We want the network to be able to learn more complicated relationships, so our activation functions must be non-linear.\n",
    "\n",
    "Some are similar in shape to the sigmoid, such as tanh. Of the dozen or so available activations in Keras, there are few that are important enough to describe in a little more detail.\n",
    "\n",
    "1. Sigmoid - we already discussed this one. It's the familiar logistic regression shape. It outputs a single value between 0 and 1.\n",
    "1. Softmax - a generalization of the sigmoid that can handle multi class outputs.\n",
    "1. ReLu (rectified linear output) - this is a very special activation function that deserves its own section.\n",
    "\n",
    "\n",
    "### Relu\n",
    "\n",
    "ReLu is defined by:\n",
    "\n",
    "$$\n",
    "\\Large\n",
    "f(x) = max(0, x)\n",
    "$$\n",
    "\n",
    "Much like another activation function called softplus, relu can output values from 0 to $+\\infty$. It has some huge advantages which make this currently the most popular activation function.\n",
    "\n",
    "1. In the cackpropagation algorithm we calculate many derivatives of the activation functions. Since the derivative of relu is either 0 or 1 (except at x = 0), this is very easy computationally. I found a source that says that model training is 6 times faster with relu than some other activation functions.\n",
    "1. Retains non-linearity.\n",
    "1. Avoids the vanishing gradient problem.\n",
    "1. One potential issue: your network can get into a state where relu neurons \"die\" or always output zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.arange(-2, 2, .01)\n",
    "relu = np.maximum(0, x)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(x, relu, color='red', lw=2)\n",
    "plt.ylim((-.5,1.5))\n",
    "plt.xlim(-2,2)\n",
    "plt.grid(True)\n",
    "plt.title(\"relu activation function\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the correct loss function and output layer activation\n",
    "\n",
    "Practically speaking, you choose these to match the machine learning task: \n",
    "\n",
    "| Problem Type                            | Activation           | Loss function  |\n",
    "| --------------------------------------- |:-------------:| -----:|\n",
    "| Binary Classifcation                    | sigmoid      |    binary_crossentropy |\n",
    "| Multiclass, single-label classification | softmax | categorical_crossentropy |\n",
    "| Multiclass, multilabel classification   | sigmoid | binary_crossentropy   |\n",
    "| Regression to arbitrary values          | none    | mse\n",
    "| Regression to values between 0 and 1    | sigmoid      | mse or binary_crossentropy |\n",
    "\n",
    "This table comes from \"Deeplearning with Python\", Francois Chollet, pg. 114."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation\n",
    "\n",
    "1. Vectorization - we need to turn our data into tensors for Keras/Tensorflow. This can include one-hot encoding, like we did on the MNIST labels.\n",
    "1. Normalization - as we saw in the example last week (dividing pixel values by 255), it's beneficial to have the input values be small (roughly 0-1). This is because we want to avoid saturating the activation functions. Neurons learn at a rate proportional to their derivative, and at very high or low values, most activation functions have very small derivatives.\n",
    "1. Padding - in some cases, like time-series data, we won't always have observations of the same length. In this case we can add extra data to even out the sequence lengths.\n",
    "1. Feature engineering - while one strength of deep learning is that it can figure out the important features for you, it can still be helpful to manually build good features. This can simplify the network or shorted training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sizing a network\n",
    "\n",
    "Much like any other machine learning model, a neural network can be underfit or overfit. One import factor in properly fitting a model is the complexity of the model. Simple models tend to be underfit; a too complex model will overfit the training data.\n",
    "\n",
    "In the language of deep learning, model complexity is often called **capacity** and is related to number of parameters. The number of parameters can easily be in the millions for our models and even billions for larger models. Adding layers or adding more neurons to already existing layers increases the number of parameters. Too little capacity and your network won't be able to learn the desired function, too much capacity and it can overfit.\n",
    "\n",
    "Unfortunately there is no equation to set the proper number of layers or the number of neurons in each layer. The recommended procedure for sizing a new model is to start with a small model. Then evaluate different architectures set the model size.\n",
    "\n",
    "## Avoiding overfit models\n",
    "\n",
    "1. Get more data! If possible, this is usually the best solution.\n",
    "1. Don't over-train! Training a model for too many epochs leads to overfitting. The model tries to minimize training loss, we are actually more concerned with test or validation loss. The image below shows an example of this using data we'll use below. ![](images/val_loss2.png)\n",
    "1. Reduce the network parameters.\n",
    "1. Dropout is a regularization method where, in each training iteration, some fraction of neurons in a layer are randomly set to zero during training. This helps break up any learned non-significant patterns that the network might be learning. I've also heard it described as effectively training an ensemble model. In much the same way that a Random Forest model improves upon a Decision Tree, this effective ensemble can decrease variance error at the cost of a little increase in bias error. In Keras, dropout is implemented as a layer: `model.add(layer.Dropout(0.5))`\n",
    " \n",
    "1. Regularize. By penalizing large weights we can force the model to be simpler. Keras has options for L1 and L2 regularization. This is exactly the same as regularization for linear or logistic regression. [Keras documention for regularization](https://keras.io/regularizers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rock paper scissors bot\n",
    "\n",
    "In the next two sessions, we are going to build a bot that plays rock-paper-scissors against us. This will involve two neural networks.\n",
    "\n",
    "1. The first network will attempt to predict a winning throw given what its opponent has thrown in the past. We'll build this one today.\n",
    "2. The second network will take in images of the player's hand and classify them as either rock, paper, or scissors. We'll do this one in the next session using a new layer type called a convolution.\n",
    "\n",
    "There will also be some other code to get the two working together that I will provide. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1. Take a look at the RPS data below. numeric_to_throw_mapping = {1: Rock, 2: Paper, 3: Scissors}. What does zero represent? There aren't many moves in the dataset. It's probably prone to overfitting. Create a \"complicated\" model and a \"simple\" model. Train both and plot the validation loss against epoches of training for both. The example code below will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2\n",
      "1,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1\n",
      "1,2,2,2,1,3,0,0,0,0,0,0,0,0,0,0,0,0,3\n",
      "1,2,2,2,1,3,3,2,1,0,0,0,0,0,0,0,0,0,3\n",
      "2,1,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2\n",
      "1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2\n",
      "1,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1\n",
      "1,2,2,2,1,3,0,0,0,0,0,0,0,0,0,0,0,0,3\n",
      "1,2,2,2,1,3,3,2,1,1,1,0,0,0,0,0,0,0,2\n",
      "1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2\n",
      "180 data/moves.csv\n"
     ]
    }
   ],
   "source": [
    "!head data/moves.csv\n",
    "!wc -l data/moves.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt(\"data/moves.csv\", delimiter=\",\")\n",
    "X = dataset[:,0:18]\n",
    "Y = to_categorical(dataset[:,18])\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=18, activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(x_train, y_train, epochs=300, batch_size=50, validation_data=(x_test, y_test), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('validation loss')\n",
    "plt.ylabel('loss (error)')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train loss', 'val loss'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2. Try to find an appropriate network configuration. Does dropout help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3. Pair up, play some RPS and add to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
